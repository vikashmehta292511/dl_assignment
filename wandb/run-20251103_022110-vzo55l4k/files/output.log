
======================================================================
Running configuration:
  activation: relu
  batch_size: 64
  dataset: fashion_mnist
  epochs: 5
  hidden_size: 128
  learning_rate: 0.001
  loss: cross_entropy
  num_layers: 3
  optimizer: sgd
  weight_decay: 0.0005
  weight_init: xavier
======================================================================
[34m[1mwandb[0m: [33mWARNING[0m Ignoring project 'dl_assignment' when running a sweep.
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
Loading data...
Training set: (54000, 784)
Validation set: (6000, 784)
Test set: (10000, 784)
Creating model...
Starting training...

Epoch 1/5
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 844/844 [00:12<00:00, 67.74it/s]
Model saved to models/best_model_vzo55l4k
Train Loss: 2.0948, Train Acc: 0.4349
Val Loss: 1.6252, Val Acc: 0.6117

Epoch 2/5
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 844/844 [00:14<00:00, 59.90it/s]
Model saved to models/best_model_vzo55l4k
Train Loss: 1.4791, Train Acc: 0.6342
Val Loss: 1.1243, Val Acc: 0.6599

Epoch 3/5
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 844/844 [00:13<00:00, 62.45it/s]
Model saved to models/best_model_vzo55l4k
Train Loss: 1.1415, Train Acc: 0.6672
Val Loss: 0.9122, Val Acc: 0.6880

Epoch 4/5
Training:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                     | 364/844 [00:05<00:07, 61.70it/s]
Traceback (most recent call last):
  File "C:\Python313\Lib\site-packages\wandb\agents\pyagent.py", line 297, in _run_job
    self._function()
    ~~~~~~~~~~~~~~^^
  File "D:\Assignment\dl_assignment\run_sweep.py", line 31, in train_sweep
    train_model(config, project_name=run.project, use_wandb=True)
    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Assignment\dl_assignment\src\train.py", line 92, in train_model
    loss, accuracy = model.train_step(
                     ~~~~~~~~~~~~~~~~^
        X_batch,
        ^^^^^^^^
    ...<2 lines>...
        loss_type=config.get('loss', 'cross_entropy')
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\Assignment\dl_assignment\src\model.py", line 191, in train_step
    optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Assignment\dl_assignment\src\model.py", line 255, in apply_gradients
    self.optimizer.apply_gradients(grads_and_vars)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\base_optimizer.py", line 462, in apply_gradients
    self.apply(grads, trainable_variables)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\base_optimizer.py", line 526, in apply
    self._backend_apply_gradients(grads, trainable_variables)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\base_optimizer.py", line 592, in _backend_apply_gradients
    self._backend_update_step(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        grads, trainable_variables, self.learning_rate
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\optimizer.py", line 120, in _backend_update_step
    tf.__internal__.distribute.interim.maybe_merge_call(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        self._distributed_tf_update_step,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        learning_rate,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\tensorflow\python\distribute\merge_call_interim.py", line 51, in maybe_merge_call
    return fn(strategy, *args, **kwargs)
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\optimizer.py", line 134, in _distributed_tf_update_step
    distribution.extended.update(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        var,
        ^^^^
    ...<2 lines>...
        group=False,
        ^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\tensorflow\python\distribute\distribute_lib.py", line 3005, in update
    return self._update(var, fn, args, kwargs, group)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\tensorflow\python\distribute\distribute_lib.py", line 4075, in _update
    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\tensorflow\python\distribute\distribute_lib.py", line 4081, in _update_non_slot
    result = fn(*args, **kwargs)
  File "C:\Python313\Lib\site-packages\tensorflow\python\autograph\impl\api.py", line 596, in wrapper
    return func(*args, **kwargs)
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\optimizer.py", line 131, in apply_grad_to_update_var
    return self.update_step(grad, var, learning_rate)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\sgd.py", line 97, in update_step
    learning_rate = ops.cast(learning_rate, variable.dtype)
  File "C:\Python313\Lib\site-packages\keras\src\ops\core.py", line 836, in cast
    return backend.core.cast(x, dtype)
           ~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\core.py", line 217, in cast
    return tf.cast(x, dtype=dtype)
           ~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\tensorflow\python\util\traceback_utils.py", line 153, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\core.py", line 84, in __tf_tensor__
    return tf.convert_to_tensor(self.value, dtype=dtype, name=name)
           ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Exception
