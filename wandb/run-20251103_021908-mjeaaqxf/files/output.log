
======================================================================
Running configuration:
  activation: tanh
  batch_size: 64
  dataset: fashion_mnist
  epochs: 5
  hidden_size: 64
  learning_rate: 0.0001
  loss: cross_entropy
  num_layers: 3
  optimizer: nadam
  weight_decay: 0.0005
  weight_init: xavier
======================================================================
[34m[1mwandb[0m: [33mWARNING[0m Ignoring project 'dl_assignment' when running a sweep.
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
Loading data...
Training set: (54000, 784)
Validation set: (6000, 784)
Test set: (10000, 784)
Creating model...
Starting training...

Epoch 1/5
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 844/844 [00:40<00:00, 20.72it/s]
Model saved to models/best_model_mjeaaqxf
Train Loss: 0.9512, Train Acc: 0.7163
Val Loss: 0.5681, Val Acc: 0.8147

Epoch 2/5
Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 844/844 [00:41<00:00, 20.55it/s]
Model saved to models/best_model_mjeaaqxf
Train Loss: 0.5825, Train Acc: 0.8267
Val Loss: 0.4684, Val Acc: 0.8349

Epoch 3/5
Training:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 598/844 [00:28<00:11, 20.80it/s]
Traceback (most recent call last):
  File "C:\Python313\Lib\site-packages\wandb\agents\pyagent.py", line 297, in _run_job
    self._function()
    ~~~~~~~~~~~~~~^^
  File "D:\Assignment\dl_assignment\run_sweep.py", line 31, in train_sweep
    train_model(config, project_name=run.project, use_wandb=True)
    ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Assignment\dl_assignment\src\train.py", line 92, in train_model
    loss, accuracy = model.train_step(
                     ~~~~~~~~~~~~~~~~^
        X_batch,
        ^^^^^^^^
    ...<2 lines>...
        loss_type=config.get('loss', 'cross_entropy')
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "D:\Assignment\dl_assignment\src\model.py", line 191, in train_step
    optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Assignment\dl_assignment\src\model.py", line 318, in apply_gradients
    self.optimizer.apply_gradients(grads_and_vars)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\base_optimizer.py", line 462, in apply_gradients
    self.apply(grads, trainable_variables)
    ~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\base_optimizer.py", line 526, in apply
    self._backend_apply_gradients(grads, trainable_variables)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\base_optimizer.py", line 592, in _backend_apply_gradients
    self._backend_update_step(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        grads, trainable_variables, self.learning_rate
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\nadam.py", line 106, in _backend_update_step
    super()._backend_update_step(grads, trainable_variables, learning_rate)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\optimizer.py", line 120, in _backend_update_step
    tf.__internal__.distribute.interim.maybe_merge_call(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        self._distributed_tf_update_step,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        learning_rate,
        ^^^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\tensorflow\python\distribute\merge_call_interim.py", line 51, in maybe_merge_call
    return fn(strategy, *args, **kwargs)
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\optimizer.py", line 134, in _distributed_tf_update_step
    distribution.extended.update(
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        var,
        ^^^^
    ...<2 lines>...
        group=False,
        ^^^^^^^^^^^^
    )
    ^
  File "C:\Python313\Lib\site-packages\tensorflow\python\distribute\distribute_lib.py", line 3005, in update
    return self._update(var, fn, args, kwargs, group)
           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\tensorflow\python\distribute\distribute_lib.py", line 4075, in _update
    return self._update_non_slot(var, fn, (var,) + tuple(args), kwargs, group)
           ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\tensorflow\python\distribute\distribute_lib.py", line 4081, in _update_non_slot
    result = fn(*args, **kwargs)
  File "C:\Python313\Lib\site-packages\tensorflow\python\autograph\impl\api.py", line 596, in wrapper
    return func(*args, **kwargs)
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\optimizer.py", line 131, in apply_grad_to_update_var
    return self.update_step(grad, var, learning_rate)
           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\optimizers\nadam.py", line 135, in update_step
    m_hat = ops.add(
        ops.divide(ops.multiply(u_t_1, m), 1 - u_product_t_1),
        ops.divide(ops.multiply(1 - u_t, gradient), 1 - u_product_t),
    )
  File "C:\Python313\Lib\site-packages\keras\src\ops\numpy.py", line 237, in add
    return backend.numpy.add(x1, x2)
           ~~~~~~~~~~~~~~~~~^^^^^^^^
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\sparse.py", line 493, in sparse_wrapper
    return func(x1, x2)
  File "C:\Python313\Lib\site-packages\keras\src\backend\tensorflow\numpy.py", line 116, in add
    and len(x1.shape) > 1
            ^^^^^^^^
  File "C:\Python313\Lib\site-packages\tensorflow\python\framework\ops.py", line 548, in shape
    self._tensor_shape = tensor_shape.TensorShape(self._shape_tuple())
                                                  ~~~~~~~~~~~~~~~~~^^
Exception
